{"cells":[{"cell_type":"markdown","id":"d40715e2","metadata":{"id":"d40715e2"},"source":["## Atención de producto punto escalado\n","\n"]},{"cell_type":"code","execution_count":null,"id":"ae6c9413","metadata":{"id":"ae6c9413"},"outputs":[],"source":["from transformers import BertModel  # Usemos un modelo BERT"]},{"cell_type":"code","execution_count":null,"id":"b5d0ddae","metadata":{"id":"b5d0ddae"},"outputs":[],"source":["model = BertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"id":"4d9b81db","metadata":{"id":"4d9b81db"},"outputs":[],"source":["len(model.encoder.layer)  # Base BERT tiene 12 codificadores en la pila de codificadores"]},{"cell_type":"code","execution_count":null,"id":"182eb9da","metadata":{"id":"182eb9da"},"outputs":[],"source":["model.encoder.layer[0]  # El primer enconder"]},{"cell_type":"code","execution_count":null,"id":"e57b1fb8","metadata":{"id":"e57b1fb8"},"outputs":[],"source":["model.encoder.layer[0].attention  # La atención en el primer encoder"]},{"cell_type":"markdown","source":["# Atención Multi-Headed"],"metadata":{"id":"12_KCa6LdF7q"},"id":"12_KCa6LdF7q"},{"cell_type":"code","source":["!pip install bertviz"],"metadata":{"id":"neDX3DbAeGgX"},"id":"neDX3DbAeGgX","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertModel, BertTokenizer\n","from bertviz import head_view\n","import torch\n","import pandas as pd"],"metadata":{"id":"2v5quB2CeOV5","executionInfo":{"status":"ok","timestamp":1716787202298,"user_tz":180,"elapsed":10963,"user":{"displayName":"Cuenta 3","userId":"12104512955486219801"}}},"id":"2v5quB2CeOV5","execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Carguemos un modelo de BERT básico.\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')"],"metadata":{"id":"QAlDvdE9eRr8"},"id":"QAlDvdE9eRr8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"Mi amiga me habló de esta clase y hasta ahora me encanta, ella tenía razón.\"\n","\n","tokens = tokenizer.encode(text)\n","inputs = torch.tensor(tokens).unsqueeze(0) # Descomprimir cambia la forma de (20,) -> (1, 20)\n","inputs"],"metadata":{"id":"RPi4GembefDF"},"id":"RPi4GembefDF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["attention = model(inputs, output_attentions=True)[2]  # Obtenga las puntuaciones de atención de BERT"],"metadata":{"id":"MU6tcuvde9gQ"},"id":"MU6tcuvde9gQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Atención media en el último encoder.\n","final_attention = attention[-1].mean(1)[0]"],"metadata":{"id":"E58Eh5U6fCaX","executionInfo":{"status":"ok","timestamp":1716787242730,"user_tz":180,"elapsed":3,"user":{"displayName":"Cuenta 3","userId":"12104512955486219801"}}},"id":"E58Eh5U6fCaX","execution_count":6,"outputs":[]},{"cell_type":"code","source":["attention_df = pd.DataFrame(final_attention.detach()).applymap(float).round(3)\n","\n","attention_df.columns = tokenizer.convert_ids_to_tokens(tokens)\n","attention_df.index = tokenizer.convert_ids_to_tokens(tokens)\n","\n","attention_df  # Las sumas entre filas suman 1. las sumas entre columnas no"],"metadata":{"id":"bfbA6DfwfMlf"},"id":"bfbA6DfwfMlf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens_as_list = tokenizer.convert_ids_to_tokens(inputs[0])\n","head_view(attention, tokens_as_list)"],"metadata":{"id":"395UG0Cffg0x"},"id":"395UG0Cffg0x","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# La cabeza 3-1 atiende al token anterior.\n","head_view(attention, tokenizer.convert_ids_to_tokens(inputs[0]), layer=2, heads=[0])"],"metadata":{"id":"cC7vRvh-f7D8"},"id":"cC7vRvh-f7D8","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cabeza 8-10 relacionando los objetos directos con sus verbos\n","head_view(attention, tokenizer.convert_ids_to_tokens(inputs[0]), layer=7, heads=[9])"],"metadata":{"id":"YZWiI1kPgG1q"},"id":"YZWiI1kPgG1q","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# atención en el décimo cabezal del octavo encoder para ver la atención directa al objeto\n","eight_ten = attention[7][0][9]"],"metadata":{"id":"MGVqCyMQgdcC","executionInfo":{"status":"ok","timestamp":1716787509154,"user_tz":180,"elapsed":362,"user":{"displayName":"Cuenta 3","userId":"12104512955486219801"}}},"id":"MGVqCyMQgdcC","execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Obtener la matriz de atención\n","attention_df = pd.DataFrame(eight_ten.detach()).applymap(float).round(3)\n","\n","attention_df.columns = tokenizer.convert_ids_to_tokens(tokens)\n","attention_df.index = tokenizer.convert_ids_to_tokens(tokens)\n","\n","attention_df"],"metadata":{"id":"EATB6Aczgo2x"},"id":"EATB6Aczgo2x","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}